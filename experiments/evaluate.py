"""
è¯„ä¼°æ¨¡å— - æ¨¡å‹æ€§èƒ½è¯„ä¼°å’Œå¯¹æ¯”åˆ†æ
"""

import numpy as np
import torch
import pandas as pd
from pathlib import Path
import sys
import os
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from models.ppo_base import PPOBase
from models.gnn_ppo import GNN_PPO
from utils.env_utils import VECEnvironmentAdapter


class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self, env_config: Dict):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            env_config: ç¯å¢ƒé…ç½®
        """
        self.env_config = env_config
        self.env = VECEnvironmentAdapter(
            num_car=env_config['num_car'],
            num_tcar=env_config['num_tcar'],
            num_scar=env_config['num_scar'], 
            num_task=env_config['num_task'],
            num_uav=env_config['num_uav'],
            num_rsu=env_config['num_rsu']
        )
        
    def evaluate_model(self, agent, num_episodes: int = 100, time_slots: int = 20) -> Dict:
        """
        è¯„ä¼°å•ä¸ªæ¨¡å‹
        
        Args:
            agent: è¦è¯„ä¼°çš„æ™ºèƒ½ä½“
            num_episodes: è¯„ä¼°å›åˆæ•°
            time_slots: æ¯å›åˆæ—¶éš™æ•°
            
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        rewards = []
        delays = []
        success_rates = []
        episode_lengths = []
        
        for episode in range(num_episodes):
            total_reward = 0.0
            state, _ = self.env.reset()
            episode_length = 0
            
            for t in range(time_slots):
                # æ ¹æ®æ™ºèƒ½ä½“ç±»å‹é€‰æ‹©åŠ¨ä½œ
                if isinstance(agent, PPOBase):
                    action, _ = agent.select_action(state)
                elif isinstance(agent, GNN_PPO):
                    # åˆ›å»ºç¯å¢ƒä¿¡æ¯ç”¨äºå›¾æ„å»º
                    _, env_info = self.env.reset()
                    action, _, _ = agent.select_action(state, env_info)
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„æ™ºèƒ½ä½“ç±»å‹: {type(agent)}")
                
                next_state, reward, done, _ = self.env.step(action)
                state = next_state
                total_reward += reward
                episode_length += 1
                
                if done:
                    break
            
            # æ”¶é›†æŒ‡æ ‡
            metrics = self.env.get_system_metrics()
            
            rewards.append(total_reward)
            delays.append(metrics['total_delay'])
            success_rates.append(metrics['success_rate'])
            episode_lengths.append(episode_length)
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        results = {
            'mean_reward': np.mean(rewards),
            'std_reward': np.std(rewards),
            'mean_delay': np.mean(delays),
            'std_delay': np.std(delays),
            'mean_success_rate': np.mean(success_rates),
            'std_success_rate': np.std(success_rates),
            'mean_episode_length': np.mean(episode_lengths),
            'rewards': rewards,
            'delays': delays,
            'success_rates': success_rates
        }
        
        return results
    
    def compare_models(self, models: Dict[str, object], num_episodes: int = 100) -> Dict:
        """
        å¯¹æ¯”å¤šä¸ªæ¨¡å‹
        
        Args:
            models: æ¨¡å‹åç§°åˆ°æ¨¡å‹å¯¹è±¡çš„æ˜ å°„
            num_episodes: è¯„ä¼°å›åˆæ•°
            
        Returns:
            å¯¹æ¯”ç»“æœ
        """
        results = {}
        
        print("ğŸ” å¼€å§‹æ¨¡å‹è¯„ä¼°å¯¹æ¯”...")
        
        for model_name, model in models.items():
            print(f"  è¯„ä¼° {model_name}...")
            results[model_name] = self.evaluate_model(model, num_episodes)
            
            # æ‰“å°ç»“æœæ‘˜è¦
            res = results[model_name]
            print(f"    å¥–åŠ±: {res['mean_reward']:.3f}Â±{res['std_reward']:.3f}")
            print(f"    å»¶è¿Ÿ: {res['mean_delay']:.2f}Â±{res['std_delay']:.2f}")
            print(f"    æˆåŠŸç‡: {res['mean_success_rate']:.3f}Â±{res['std_success_rate']:.3f}")
        
        return results
    
    def statistical_significance_test(self, results1: List[float], results2: List[float]) -> Dict:
        """
        ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
        
        Args:
            results1: ç¬¬ä¸€ç»„ç»“æœ
            results2: ç¬¬äºŒç»„ç»“æœ
            
        Returns:
            æ£€éªŒç»“æœ
        """
        from scipy import stats
        
        # tæ£€éªŒ
        t_stat, t_pvalue = stats.ttest_ind(results1, results2)
        
        # Mann-Whitney Uæ£€éªŒ (éå‚æ•°æ£€éªŒ)
        u_stat, u_pvalue = stats.mannwhitneyu(results1, results2, alternative='two-sided')
        
        # æ•ˆåº”é‡ (Cohen's d)
        pooled_std = np.sqrt((np.var(results1) + np.var(results2)) / 2)
        cohens_d = (np.mean(results1) - np.mean(results2)) / pooled_std if pooled_std > 0 else 0
        
        return {
            't_statistic': t_stat,
            't_pvalue': t_pvalue,
            'u_statistic': u_stat, 
            'u_pvalue': u_pvalue,
            'cohens_d': cohens_d,
            'is_significant': t_pvalue < 0.05
        }


def load_trained_models(model_paths: Dict[str, str]) -> Dict[str, object]:
    """
    åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    
    Args:
        model_paths: æ¨¡å‹åç§°åˆ°è·¯å¾„çš„æ˜ å°„
        
    Returns:
        åŠ è½½çš„æ¨¡å‹å­—å…¸
    """
    models = {}
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    for model_name, model_path in model_paths.items():
        if not os.path.exists(model_path):
            print(f"âš ï¸  æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            continue
            
        try:
            if 'baseline' in model_name.lower() or 'ppo' in model_name.lower():
                # PPOåŸºçº¿æ¨¡å‹
                model = PPOBase(
                    state_dim=64,  # æ ¹æ®å®é™…é…ç½®è°ƒæ•´
                    action_dim=15,
                    device=device
                )
                model.load_model(model_path)
                
            elif 'gnn' in model_name.lower():
                # GNN-PPOæ¨¡å‹
                model = GNN_PPO()
                model.load(model_path)
            
            else:
                print(f"âš ï¸  æœªçŸ¥æ¨¡å‹ç±»å‹: {model_name}")
                continue
                
            models[model_name] = model
            print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {model_name}")
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥ {model_name}: {e}")
    
    return models


def create_evaluation_report(comparison_results: Dict, output_path: str = 'plots/evaluation_report.txt'):
    """
    åˆ›å»ºè¯„ä¼°æŠ¥å‘Š
    
    Args:
        comparison_results: å¯¹æ¯”ç»“æœ
        output_path: è¾“å‡ºè·¯å¾„
    """
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("GNN-PPO vs PPO Baseline æ¨¡å‹è¯„ä¼°æŠ¥å‘Š\\n")
        f.write("="*50 + "\\n\\n")
        f.write(f"è¯„ä¼°æ—¶é—´: {pd.Timestamp.now()}\\n")
        f.write(f"è¯„ä¼°å›åˆæ•°: 100\\n\\n")
        
        # è¯¦ç»†ç»Ÿè®¡
        f.write("è¯¦ç»†ç»Ÿè®¡æŒ‡æ ‡:\\n")
        f.write("-" * 30 + "\\n")
        
        for model_name, results in comparison_results.items():
            f.write(f"\\n{model_name}:\\n")
            f.write(f"  å¹³å‡å¥–åŠ±: {results['mean_reward']:.4f} Â± {results['std_reward']:.4f}\\n")
            f.write(f"  å¹³å‡å»¶è¿Ÿ: {results['mean_delay']:.4f} Â± {results['std_delay']:.4f}\\n")
            f.write(f"  å¹³å‡æˆåŠŸç‡: {results['mean_success_rate']:.4f} Â± {results['std_success_rate']:.4f}\\n")
            f.write(f"  å¹³å‡å›åˆé•¿åº¦: {results['mean_episode_length']:.2f}\\n")
        
        # å¦‚æœæœ‰ä¸¤ä¸ªæ¨¡å‹ï¼Œè¿›è¡Œç»Ÿè®¡æ£€éªŒ
        if len(comparison_results) == 2:
            models = list(comparison_results.keys())
            model1, model2 = models[0], models[1]
            
            # åˆ›å»ºè¯„ä¼°å™¨è¿›è¡Œç»Ÿè®¡æ£€éªŒ
            evaluator = ModelEvaluator({
                'num_car': 20, 'num_tcar': 15, 'num_scar': 5,
                'num_task': 15, 'num_uav': 1, 'num_rsu': 1
            })
            
            # å¥–åŠ±æ˜¾è‘—æ€§æ£€éªŒ
            reward_test = evaluator.statistical_significance_test(
                comparison_results[model1]['rewards'],
                comparison_results[model2]['rewards']
            )
            
            # å»¶è¿Ÿæ˜¾è‘—æ€§æ£€éªŒ  
            delay_test = evaluator.statistical_significance_test(
                comparison_results[model1]['delays'],
                comparison_results[model2]['delays']
            )
            
            f.write("\\n\\nç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ:\\n")
            f.write("-" * 30 + "\\n")
            f.write(f"å¥–åŠ±å·®å¼‚ ({model1} vs {model2}):\\n")
            f.write(f"  tç»Ÿè®¡é‡: {reward_test['t_statistic']:.4f}\\n")
            f.write(f"  på€¼: {reward_test['t_pvalue']:.6f}\\n")
            f.write(f"  Cohen's d: {reward_test['cohens_d']:.4f}\\n")
            f.write(f"  æ˜¯å¦æ˜¾è‘—: {'æ˜¯' if reward_test['is_significant'] else 'å¦'}\\n")
            
            f.write(f"\\nå»¶è¿Ÿå·®å¼‚ ({model1} vs {model2}):\\n")
            f.write(f"  tç»Ÿè®¡é‡: {delay_test['t_statistic']:.4f}\\n")
            f.write(f"  på€¼: {delay_test['t_pvalue']:.6f}\\n")
            f.write(f"  Cohen's d: {delay_test['cohens_d']:.4f}\\n")
            f.write(f"  æ˜¯å¦æ˜¾è‘—: {'æ˜¯' if delay_test['is_significant'] else 'å¦'}\\n")
    
    print(f"ğŸ“„ è¯„ä¼°æŠ¥å‘Šä¿å­˜è‡³: {output_path}")


def evaluate_models():
    """è¯„ä¼°å·²è®­ç»ƒçš„æ¨¡å‹"""
    
    # å®šä¹‰æ¨¡å‹è·¯å¾„
    model_paths = {
        'PPO Baseline': 'saved_models/PPO_baseline_final.pth',
        'GNN-PPO': 'saved_models/GNN_PPO_model.pth'
    }
    
    # ç¯å¢ƒé…ç½®
    env_config = {
        'num_car': 20,
        'num_tcar': 15,
        'num_scar': 5,
        'num_task': 15,
        'num_uav': 1,
        'num_rsu': 1
    }
    
    print("ğŸš€ å¼€å§‹æ¨¡å‹è¯„ä¼°...")
    
    # åŠ è½½æ¨¡å‹ (å¦‚æœå­˜åœ¨çš„è¯)
    models = {}
    
    # ç”±äºå¯èƒ½æ²¡æœ‰è®­ç»ƒå¥½çš„åŸºçº¿æ¨¡å‹ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç”¨äºæ¼”ç¤º
    try:
        # å°è¯•åŠ è½½GNN-PPOæ¨¡å‹
        if os.path.exists(model_paths['GNN-PPO']):
            gnn_ppo = GNN_PPO()
            gnn_ppo.load(model_paths['GNN-PPO'])
            models['GNN-PPO'] = gnn_ppo
            print("âœ… æˆåŠŸåŠ è½½GNN-PPOæ¨¡å‹")
        
        # åˆ›å»ºæœªè®­ç»ƒçš„åŸºçº¿PPOç”¨äºå¯¹æ¯”æ¼”ç¤º
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        baseline_ppo = PPOBase(
            state_dim=64,
            action_dim=15, 
            device=device
        )
        models['PPO Baseline (Untrained)'] = baseline_ppo
        print("âœ… åˆ›å»ºPPOåŸºçº¿æ¨¡å‹ï¼ˆæœªè®­ç»ƒï¼‰")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    if not models:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹è¿›è¡Œè¯„ä¼°")
        return
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = ModelEvaluator(env_config)
    
    # è¯„ä¼°æ¨¡å‹
    results = evaluator.compare_models(models, num_episodes=50)  # å‡å°‘å›åˆæ•°ä»¥èŠ‚çœæ—¶é—´
    
    # åˆ›å»ºè¯„ä¼°æŠ¥å‘Š
    create_evaluation_report(results)
    
    print("\\nğŸ“Š è¯„ä¼°å®Œæˆï¼")
    return results


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ¨¡å‹è¯„ä¼°')
    parser.add_argument('--num_episodes', type=int, default=100, help='è¯„ä¼°å›åˆæ•°')
    parser.add_argument('--output_dir', type=str, default='plots/', help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # è¿è¡Œè¯„ä¼°
    results = evaluate_models()
    
    if results:
        print("\\nâœ… è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°plots/ç›®å½•")


if __name__ == '__main__':
    main()