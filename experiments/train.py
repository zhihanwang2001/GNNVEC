"""
è®­ç»ƒè„šæœ¬ - æ”¯æŒåŸºçº¿å¯¹æ¯”å’ŒGNN-PPOè®­ç»ƒ
"""

import argparse
import sys
import os
import numpy as np
import torch
import yaml
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from models.ppo_base import PPOBase, Transition
from models.gnn_ppo import GNN_PPO, Transition as GNNTransition
from utils.env_utils import VECEnvironmentAdapter


def train_baseline_ppo(config: dict, log_file: str = None):
    """
    è®­ç»ƒåŸºçº¿PPOç®—æ³•
    
    Args:
        config: é…ç½®å­—å…¸
        log_file: æ—¥å¿—æ–‡ä»¶è·¯å¾„
    """
    print("=" * 60)
    print("è®­ç»ƒåŸºçº¿PPOç®—æ³•")
    print("=" * 60)
    
    # ç¯å¢ƒè®¾ç½®
    env_config = config['environment']
    env = VECEnvironmentAdapter(
        num_car=env_config['num_car'],
        num_tcar=env_config['num_tcar'], 
        num_scar=env_config['num_scar'],
        num_task=env_config['num_task'],
        num_uav=env_config['num_uav'],
        num_rsu=env_config['num_rsu']
    )
    
    # è®¾ç½®éšæœºç§å­
    if config['experiment']['seed']:
        seed = config['experiment']['random_seed']
        env.seed(seed)
        torch.manual_seed(seed)
        np.random.seed(seed)
    
    # åˆå§‹åŒ–PPOæ™ºèƒ½ä½“
    train_config = config['training']
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    agent = PPOBase(
        state_dim=env.state_space_shape[0],
        action_dim=env.action_space_shape[0],
        lr=float(train_config['learning_rate']),
        gamma=float(train_config['gamma']),
        clip_param=float(train_config['clip_param']),
        buffer_capacity=int(train_config['capacity']),
        batch_size=int(train_config['batch_size']),
        update_iteration=int(train_config['update_iteration']),
        max_grad_norm=float(train_config['max_grad_norm']),
        device=device
    )
    
    # è®­ç»ƒè®°å½•
    reward_record = []
    delay_record = []
    rate_record = []
    
    # æ—¥å¿—æ–‡ä»¶
    if log_file is None:
        log_file = f'experiment_plot_new/PPO_baseline_{env_config["num_tcar"]}_{env_config["num_scar"]}.log'
    
    os.makedirs(os.path.dirname(log_file), exist_ok=True)
    f = open(log_file, 'w')
    print('episode', 'mean_reward', 'mean_delay', 'mean_rate', file=f)
    
    # è®­ç»ƒå¾ªç¯
    episodes = int(train_config['episodes'])
    time_slots = int(env_config['time_slots'])
    
    for episode in range(episodes):
        total_reward = 0.0
        state, _ = env.reset()
        
        for t in range(time_slots):
            # é€‰æ‹©åŠ¨ä½œ
            action, action_log_prob = agent.select_action(state)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, _ = env.step(action)
            
            # å­˜å‚¨ç»éªŒ
            transition = Transition(state, action, action_log_prob, reward, next_state)
            should_update = agent.store_transition(transition)
            
            # æ›´æ–°ç½‘ç»œ
            if should_update:
                agent.update()
            
            state = next_state
            total_reward += reward
            
            if done:
                break
        
        # è·å–ç³»ç»ŸæŒ‡æ ‡
        metrics = env.get_system_metrics()
        total_delay = metrics['total_delay']
        total_success_rate = metrics['success_rate']
        
        # è®°å½•ç»“æœ
        reward_record.append(total_reward)
        delay_record.append(total_delay)
        rate_record.append(total_success_rate)
        
        # è®¡ç®—ç§»åŠ¨å¹³å‡
        window = min(100, episode + 1)
        reward_avg = np.mean(reward_record[-window:])
        delay_avg = np.mean(delay_record[-window:])
        rate_avg = np.mean(rate_record[-window:])
        
        # è¾“å‡ºè¿›åº¦
        if episode % int(config['experiment']['log_interval']) == 0:
            print(f"Episode: {episode}, Reward: {reward_avg:.2f}, "
                  f"Delay: {delay_avg:.2f}, Success Rate: {rate_avg:.3f}")
        
        # å†™å…¥æ—¥å¿—
        print(episode, reward_avg, delay_avg, rate_avg, file=f)
        
        # å®šæœŸä¿å­˜æ¨¡å‹
        if episode % int(train_config['save_interval']) == 0 and episode > 0:
            model_path = f'saved_models/PPO_baseline_ep{episode}.pth'
            agent.save_model(model_path)
    
    f.close()
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = 'saved_models/PPO_baseline_final.pth'
    agent.save_model(final_model_path)
    
    print(f"åŸºçº¿PPOè®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜è‡³: {final_model_path}")
    print(f"è®­ç»ƒæ—¥å¿—ä¿å­˜è‡³: {log_file}")
    
    return {
        'reward_record': reward_record,
        'delay_record': delay_record,
        'rate_record': rate_record,
        'final_metrics': {
            'avg_reward': np.mean(reward_record[-100:]),
            'avg_delay': np.mean(delay_record[-100:]),
            'avg_rate': np.mean(rate_record[-100:])
        }
    }


def train_gnn_ppo(config: dict, log_file: str = None):
    """
    è®­ç»ƒGNNå¢å¼ºPPOç®—æ³•
    
    Args:
        config: é…ç½®å­—å…¸
        log_file: æ—¥å¿—æ–‡ä»¶è·¯å¾„
    """
    print("=" * 60)
    print("è®­ç»ƒGNNå¢å¼ºPPOç®—æ³•")
    print("=" * 60)
    
    # è°ƒç”¨ç°æœ‰çš„GNN_PPOè®­ç»ƒé€»è¾‘
    from models.gnn_ppo import main as gnn_ppo_main
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['GNN_PPO_CONFIG'] = yaml.dump(config)
    
    # æ„å»ºå‚æ•°
    original_argv = sys.argv
    sys.argv = [
        'gnn_ppo.py',
        '--mode', 'train',
        '--gnn_type', config['gnn']['type'],
        '--learning_rate', str(config['training']['learning_rate']),
        '--batch_size', str(config['training']['batch_size']),
        '--gnn_hidden_dim', str(config['gnn']['hidden_dim']),
        '--gnn_output_dim', str(config['gnn']['output_dim'])
    ]
    
    try:
        gnn_ppo_main()
    finally:
        sys.argv = original_argv
    
    print("GNN-PPOè®­ç»ƒå®Œæˆï¼")


def compare_training_runs():
    """
    è¿è¡Œå¯¹æ¯”è®­ç»ƒå®éªŒ
    """
    print("=" * 80)
    print("å¼€å§‹å¯¹æ¯”è®­ç»ƒå®éªŒï¼šPPOåŸºçº¿ vs GNN-PPO")
    print("=" * 80)
    
    # åŠ è½½é…ç½®
    baseline_config_path = Path(__file__).parent.parent / 'configs' / 'ppo_baseline.yaml'
    gnn_config_path = Path(__file__).parent.parent / 'configs' / 'default.yaml'
    
    with open(baseline_config_path, 'r') as f:
        baseline_config = yaml.safe_load(f)
    
    with open(gnn_config_path, 'r') as f:
        gnn_config = yaml.safe_load(f)
    
    # ç¡®ä¿ä½¿ç”¨ç›¸åŒçš„éšæœºç§å­
    baseline_config['experiment']['random_seed'] = 9527
    gnn_config['experiment']['random_seed'] = 9527
    
    results = {}
    
    # è®­ç»ƒåŸºçº¿PPO
    print("\nğŸ”µ æ­¥éª¤1: è®­ç»ƒåŸºçº¿PPO...")
    baseline_results = train_baseline_ppo(
        baseline_config, 
        'experiment_plot_new/comparison_PPO_baseline.log'
    )
    results['baseline'] = baseline_results
    
    # è®­ç»ƒGNN-PPO
    print("\nğŸŸ¢ æ­¥éª¤2: è®­ç»ƒGNN-PPO...")
    train_gnn_ppo(
        gnn_config,
        'experiment_plot_new/comparison_GNN_PPO.log'  
    )
    
    print("\nâœ… å¯¹æ¯”è®­ç»ƒå®Œæˆï¼")
    print("ğŸ“Š å¯ä»¥ä½¿ç”¨visualize.pyç”Ÿæˆå¯¹æ¯”å›¾è¡¨")
    
    return results


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è®­ç»ƒè„šæœ¬')
    parser.add_argument('--algorithm', type=str, default='compare', 
                       choices=['baseline', 'gnn_ppo', 'compare'],
                       help='è®­ç»ƒç®—æ³•ç±»å‹')
    parser.add_argument('--config', type=str, default=None, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    if args.algorithm == 'compare':
        compare_training_runs()
    elif args.algorithm == 'baseline':
        config_path = args.config or 'configs/ppo_baseline.yaml'
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        train_baseline_ppo(config)
    elif args.algorithm == 'gnn_ppo':
        config_path = args.config or 'configs/default.yaml'
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        train_gnn_ppo(config)


if __name__ == '__main__':
    main()