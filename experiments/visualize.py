"""
å¯è§†åŒ–æ¨¡å— - ç”Ÿæˆè®­ç»ƒæ›²çº¿å’Œæ€§èƒ½å¯¹æ¯”å›¾è¡¨
"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from pathlib import Path
import argparse
from typing import List, Dict, Tuple
import os

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œæ ·å¼
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")
sns.set_palette("husl")


def load_training_log(log_file: str) -> pd.DataFrame:
    """
    åŠ è½½è®­ç»ƒæ—¥å¿—æ–‡ä»¶
    
    Args:
        log_file: æ—¥å¿—æ–‡ä»¶è·¯å¾„
        
    Returns:
        åŒ…å«è®­ç»ƒæ•°æ®çš„DataFrame
    """
    try:
        df = pd.read_csv(log_file, sep=' ')
        return df
    except Exception as e:
        print(f"åŠ è½½æ—¥å¿—æ–‡ä»¶å¤±è´¥: {log_file}, é”™è¯¯: {e}")
        return None


def plot_training_curves(log_files: Dict[str, str], output_dir: str = 'plots/'):
    """
    ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    
    Args:
        log_files: ç®—æ³•åç§°åˆ°æ—¥å¿—æ–‡ä»¶è·¯å¾„çš„æ˜ å°„
        output_dir: è¾“å‡ºç›®å½•
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # åˆ›å»ºå›¾è¡¨
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('GNN-PPO vs PPO Baseline Training Comparison', fontsize=16, fontweight='bold')
    
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']
    
    for i, (alg_name, log_file) in enumerate(log_files.items()):
        df = load_training_log(log_file)
        if df is None:
            continue
            
        color = colors[i % len(colors)]
        
        # 1. å¥–åŠ±æ›²çº¿
        axes[0, 0].plot(df['episode'], df['mean_reward'], 
                       label=alg_name, color=color, linewidth=2)
        axes[0, 0].set_title('Training Reward', fontweight='bold')
        axes[0, 0].set_xlabel('Episode')
        axes[0, 0].set_ylabel('Average Reward')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. å»¶è¿Ÿæ›²çº¿
        axes[0, 1].plot(df['episode'], df['mean_delay'], 
                       label=alg_name, color=color, linewidth=2)
        axes[0, 1].set_title('Average Delay', fontweight='bold')
        axes[0, 1].set_xlabel('Episode')
        axes[0, 1].set_ylabel('Delay (ms)')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. æˆåŠŸç‡æ›²çº¿
        axes[1, 0].plot(df['episode'], df['mean_rate'], 
                       label=alg_name, color=color, linewidth=2)
        axes[1, 0].set_title('Success Rate', fontweight='bold')
        axes[1, 0].set_xlabel('Episode')
        axes[1, 0].set_ylabel('Success Rate')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. æ”¶æ•›æ€§åˆ†æ - ç§»åŠ¨å¹³å‡
        window = 50
        smooth_reward = df['mean_reward'].rolling(window=window).mean()
        axes[1, 1].plot(df['episode'], smooth_reward, 
                       label=f'{alg_name} (MA-{window})', color=color, linewidth=2)
    
    axes[1, 1].set_title('Convergence Analysis', fontweight='bold')
    axes[1, 1].set_xlabel('Episode')
    axes[1, 1].set_ylabel('Smoothed Reward')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    plot_path = os.path.join(output_dir, 'training_comparison.png')
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    print(f"è®­ç»ƒå¯¹æ¯”å›¾ä¿å­˜è‡³: {plot_path}")
    
    plt.show()


def plot_performance_comparison(log_files: Dict[str, str], output_dir: str = 'plots/'):
    """
    ç»˜åˆ¶æ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾
    
    Args:
        log_files: ç®—æ³•åç§°åˆ°æ—¥å¿—æ–‡ä»¶è·¯å¾„çš„æ˜ å°„
        output_dir: è¾“å‡ºç›®å½•
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # æ”¶é›†æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡
    performance_data = []
    
    for alg_name, log_file in log_files.items():
        df = load_training_log(log_file)
        if df is None:
            continue
            
        # å–æœ€å100ä¸ªepisodeçš„å¹³å‡å€¼
        final_metrics = {
            'Algorithm': alg_name,
            'Final Reward': df['mean_reward'].tail(100).mean(),
            'Final Delay': df['mean_delay'].tail(100).mean(),
            'Final Success Rate': df['mean_rate'].tail(100).mean(),
        }
        performance_data.append(final_metrics)
    
    if not performance_data:
        print("æ²¡æœ‰å¯ç”¨çš„æ€§èƒ½æ•°æ®")
        return
    
    perf_df = pd.DataFrame(performance_data)
    
    # åˆ›å»ºå¯¹æ¯”æŸ±çŠ¶å›¾
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    fig.suptitle('Final Performance Comparison (Last 100 Episodes Average)', 
                fontsize=16, fontweight='bold')
    
    # 1. å¥–åŠ±å¯¹æ¯”
    bars1 = axes[0].bar(perf_df['Algorithm'], perf_df['Final Reward'], 
                       color=['#1f77b4', '#ff7f0e'], alpha=0.8)
    axes[0].set_title('Final Average Reward', fontweight='bold')
    axes[0].set_ylabel('Reward')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars1, perf_df['Final Reward']):
        axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{value:.2f}', ha='center', va='bottom', fontweight='bold')
    
    # 2. å»¶è¿Ÿå¯¹æ¯”
    bars2 = axes[1].bar(perf_df['Algorithm'], perf_df['Final Delay'], 
                       color=['#1f77b4', '#ff7f0e'], alpha=0.8)
    axes[1].set_title('Final Average Delay', fontweight='bold')
    axes[1].set_ylabel('Delay (ms)')
    
    for bar, value in zip(bars2, perf_df['Final Delay']):
        axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                    f'{value:.1f}', ha='center', va='bottom', fontweight='bold')
    
    # 3. æˆåŠŸç‡å¯¹æ¯”
    bars3 = axes[2].bar(perf_df['Algorithm'], perf_df['Final Success Rate'], 
                       color=['#1f77b4', '#ff7f0e'], alpha=0.8)
    axes[2].set_title('Final Success Rate', fontweight='bold')
    axes[2].set_ylabel('Success Rate')
    
    for bar, value in zip(bars3, perf_df['Final Success Rate']):
        axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
                    f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    plot_path = os.path.join(output_dir, 'performance_comparison.png')
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    print(f"æ€§èƒ½å¯¹æ¯”å›¾ä¿å­˜è‡³: {plot_path}")
    
    # è®¡ç®—æ”¹è¿›ç™¾åˆ†æ¯”
    if len(performance_data) >= 2:
        baseline = performance_data[0]  # å‡è®¾ç¬¬ä¸€ä¸ªæ˜¯åŸºçº¿
        gnn_ppo = performance_data[1]   # ç¬¬äºŒä¸ªæ˜¯GNN-PPO
        
        reward_improvement = (gnn_ppo['Final Reward'] - baseline['Final Reward']) / abs(baseline['Final Reward']) * 100
        delay_improvement = (baseline['Final Delay'] - gnn_ppo['Final Delay']) / baseline['Final Delay'] * 100
        rate_improvement = (gnn_ppo['Final Success Rate'] - baseline['Final Success Rate']) / baseline['Final Success Rate'] * 100
        
        print(f"\\nğŸ“Š GNN-PPOç›¸æ¯”PPOåŸºçº¿çš„æ”¹è¿›:")
        print(f"ğŸ¯ å¥–åŠ±æ”¹è¿›: {reward_improvement:+.1f}%")
        print(f"âš¡ å»¶è¿Ÿé™ä½: {delay_improvement:+.1f}%")
        print(f"âœ… æˆåŠŸç‡æå‡: {rate_improvement:+.1f}%")
    
    plt.show()


def plot_convergence_analysis(log_files: Dict[str, str], output_dir: str = 'plots/'):
    """
    ç»˜åˆ¶è¯¦ç»†çš„æ”¶æ•›æ€§åˆ†æå›¾
    
    Args:
        log_files: ç®—æ³•åç§°åˆ°æ—¥å¿—æ–‡ä»¶è·¯å¾„çš„æ˜ å°„
        output_dir: è¾“å‡ºç›®å½•
    """
    os.makedirs(output_dir, exist_ok=True)
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('Convergence Analysis: GNN-PPO vs PPO Baseline', 
                fontsize=16, fontweight='bold')
    
    for alg_name, log_file in log_files.items():
        df = load_training_log(log_file)
        if df is None:
            continue
        
        color = '#1f77b4' if 'baseline' in alg_name.lower() else '#ff7f0e'
        
        # 1. å¥–åŠ±æ–¹å·®åˆ†æ
        window = 50
        rolling_std = df['mean_reward'].rolling(window=window).std()
        axes[0, 0].plot(df['episode'], rolling_std, label=f'{alg_name} Variance', 
                       color=color, linewidth=2)
        axes[0, 0].set_title('Reward Variance (Stability)', fontweight='bold')
        axes[0, 0].set_xlabel('Episode')
        axes[0, 0].set_ylabel('Rolling Std')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. å­¦ä¹ ç‡åˆ†æ (å¥–åŠ±å˜åŒ–ç‡)
        reward_diff = df['mean_reward'].diff().abs()
        smooth_diff = reward_diff.rolling(window=window).mean()
        axes[0, 1].plot(df['episode'][1:], smooth_diff[1:], label=f'{alg_name} Learning Rate', 
                       color=color, linewidth=2)
        axes[0, 1].set_title('Learning Rate (Reward Change)', fontweight='bold')
        axes[0, 1].set_xlabel('Episode')
        axes[0, 1].set_ylabel('Abs Reward Change')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. ç´¯ç§¯æ€§èƒ½åˆ†æ
        cumulative_reward = df['mean_reward'].cumsum()
        axes[1, 0].plot(df['episode'], cumulative_reward, label=f'{alg_name} Cumulative', 
                       color=color, linewidth=2)
        axes[1, 0].set_title('Cumulative Reward', fontweight='bold')
        axes[1, 0].set_xlabel('Episode')
        axes[1, 0].set_ylabel('Cumulative Reward')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. å¤šæŒ‡æ ‡ç»¼åˆå¾—åˆ†
        # æ ‡å‡†åŒ–å„æŒ‡æ ‡åˆ°0-1èŒƒå›´
        norm_reward = (df['mean_reward'] - df['mean_reward'].min()) / (df['mean_reward'].max() - df['mean_reward'].min())
        norm_delay = 1 - (df['mean_delay'] - df['mean_delay'].min()) / (df['mean_delay'].max() - df['mean_delay'].min())  # å»¶è¿Ÿè¶Šä½è¶Šå¥½
        norm_rate = (df['mean_rate'] - df['mean_rate'].min()) / (df['mean_rate'].max() - df['mean_rate'].min())
        
        # ç»¼åˆå¾—åˆ† (æƒé‡: å¥–åŠ±40%, å»¶è¿Ÿ30%, æˆåŠŸç‡30%)
        composite_score = 0.4 * norm_reward + 0.3 * norm_delay + 0.3 * norm_rate
        smooth_score = composite_score.rolling(window=window).mean()
        
        axes[1, 1].plot(df['episode'], smooth_score, label=f'{alg_name} Composite', 
                       color=color, linewidth=2)
        axes[1, 1].set_title('Composite Performance Score', fontweight='bold')
        axes[1, 1].set_xlabel('Episode')
        axes[1, 1].set_ylabel('Normalized Score')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    plot_path = os.path.join(output_dir, 'convergence_analysis.png')
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    print(f"æ”¶æ•›æ€§åˆ†æå›¾ä¿å­˜è‡³: {plot_path}")
    
    plt.show()


def create_summary_report(log_files: Dict[str, str], output_dir: str = 'plots/'):
    """
    åˆ›å»ºå®éªŒæ€»ç»“æŠ¥å‘Š
    
    Args:
        log_files: ç®—æ³•åç§°åˆ°æ—¥å¿—æ–‡ä»¶è·¯å¾„çš„æ˜ å°„
        output_dir: è¾“å‡ºç›®å½•
    """
    os.makedirs(output_dir, exist_ok=True)
    
    print("\\n" + "="*80)
    print("ğŸ“Š GNN-PPO vs PPO Baseline å®éªŒæ€»ç»“æŠ¥å‘Š")
    print("="*80)
    
    summary_data = []
    
    for alg_name, log_file in log_files.items():
        df = load_training_log(log_file)
        if df is None:
            continue
        
        # è®¡ç®—å…³é”®ç»Ÿè®¡æŒ‡æ ‡
        final_100 = df.tail(100)
        
        stats = {
            'ç®—æ³•': alg_name,
            'æœ€ç»ˆå¹³å‡å¥–åŠ±': final_100['mean_reward'].mean(),
            'æœ€ç»ˆå¹³å‡å»¶è¿Ÿ': final_100['mean_delay'].mean(),
            'æœ€ç»ˆæˆåŠŸç‡': final_100['mean_rate'].mean(),
            'å¥–åŠ±æ ‡å‡†å·®': final_100['mean_reward'].std(),
            'æœ€ä½³å¥–åŠ±': df['mean_reward'].max(),
            'æœ€ä½å»¶è¿Ÿ': df['mean_delay'].min(),
            'æœ€é«˜æˆåŠŸç‡': df['mean_rate'].max(),
            'æ”¶æ•›episode': len(df)
        }
        
        summary_data.append(stats)
    
    # æ‰“å°å¯¹æ¯”è¡¨æ ¼
    if len(summary_data) >= 2:
        baseline = summary_data[0]
        gnn_ppo = summary_data[1]
        
        print(f"\\n{'æŒ‡æ ‡':<20} {'PPOåŸºçº¿':<15} {'GNN-PPO':<15} {'æ”¹è¿›å¹…åº¦':<15}")
        print("-" * 70)
        
        metrics = [
            ('æœ€ç»ˆå¹³å‡å¥–åŠ±', 'mean_reward'),
            ('æœ€ç»ˆå¹³å‡å»¶è¿Ÿ', 'mean_delay'), 
            ('æœ€ç»ˆæˆåŠŸç‡', 'mean_rate'),
            ('å¥–åŠ±ç¨³å®šæ€§', 'reward_std')
        ]
        
        improvements = {}
        
        for metric_name, key in metrics:
            if key == 'mean_delay':
                # å»¶è¿Ÿè¶Šä½è¶Šå¥½
                baseline_val = baseline['æœ€ç»ˆå¹³å‡å»¶è¿Ÿ']
                gnn_val = gnn_ppo['æœ€ç»ˆå¹³å‡å»¶è¿Ÿ']
                improvement = (baseline_val - gnn_val) / baseline_val * 100
                improvements[metric_name] = improvement
                print(f"{metric_name:<20} {baseline_val:<15.2f} {gnn_val:<15.2f} {improvement:<15.1f}%")
            elif key == 'reward_std':
                # æ ‡å‡†å·®è¶Šå°è¶Šç¨³å®š
                baseline_val = baseline['å¥–åŠ±æ ‡å‡†å·®']
                gnn_val = gnn_ppo['å¥–åŠ±æ ‡å‡†å·®']
                improvement = (baseline_val - gnn_val) / baseline_val * 100
                improvements[metric_name] = improvement
                print(f"{metric_name:<20} {baseline_val:<15.3f} {gnn_val:<15.3f} {improvement:<15.1f}%")
            else:
                # å…¶ä»–æŒ‡æ ‡è¶Šå¤§è¶Šå¥½
                baseline_val = baseline[f'æœ€ç»ˆ{metric_name[2:]}'] if metric_name.startswith('æœ€ç»ˆ') else baseline[metric_name]
                gnn_val = gnn_ppo[f'æœ€ç»ˆ{metric_name[2:]}'] if metric_name.startswith('æœ€ç»ˆ') else gnn_ppo[metric_name]
                improvement = (gnn_val - baseline_val) / abs(baseline_val) * 100
                improvements[metric_name] = improvement
                print(f"{metric_name:<20} {baseline_val:<15.3f} {gnn_val:<15.3f} {improvement:<15.1f}%")
        
        print("\\nğŸ¯ å…³é”®å‘ç°:")
        print(f"â€¢ GNN-PPOåœ¨å¥–åŠ±ä¸Š{'ä¼˜äº' if improvements.get('æœ€ç»ˆå¹³å‡å¥–åŠ±', 0) > 0 else 'ä¸å¦‚'}PPOåŸºçº¿ ({improvements.get('æœ€ç»ˆå¹³å‡å¥–åŠ±', 0):.1f}%)")
        print(f"â€¢ GNN-PPOåœ¨å»¶è¿Ÿä¸Š{'ä¼˜äº' if improvements.get('æœ€ç»ˆå¹³å‡å»¶è¿Ÿ', 0) > 0 else 'ä¸å¦‚'}PPOåŸºçº¿ ({improvements.get('æœ€ç»ˆå¹³å‡å»¶è¿Ÿ', 0):.1f}%)")
        print(f"â€¢ GNN-PPOåœ¨æˆåŠŸç‡ä¸Š{'ä¼˜äº' if improvements.get('æœ€ç»ˆæˆåŠŸç‡', 0) > 0 else 'ä¸å¦‚'}PPOåŸºçº¿ ({improvements.get('æœ€ç»ˆæˆåŠŸç‡', 0):.1f}%)")
        print(f"â€¢ GNN-PPO{'æ›´' if improvements.get('å¥–åŠ±ç¨³å®šæ€§', 0) > 0 else 'ä¸å¤Ÿ'}ç¨³å®š ({improvements.get('å¥–åŠ±ç¨³å®šæ€§', 0):.1f}%)")
    
    # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
    report_path = os.path.join(output_dir, 'experiment_summary.txt')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("GNN-PPO vs PPO Baseline å®éªŒæ€»ç»“æŠ¥å‘Š\\n")
        f.write("="*50 + "\\n\\n")
        
        for stats in summary_data:
            f.write(f"ç®—æ³•: {stats['ç®—æ³•']}\\n")
            for key, value in stats.items():
                if key != 'ç®—æ³•':
                    f.write(f"  {key}: {value:.4f}\\n")
            f.write("\\n")
    
    print(f"\\nğŸ“ è¯¦ç»†æŠ¥å‘Šä¿å­˜è‡³: {report_path}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¯è§†åŒ–è®­ç»ƒç»“æœ')
    parser.add_argument('--baseline_log', type=str, 
                       default='experiment_plot_new/PPO_baseline_15_5.log',
                       help='PPOåŸºçº¿æ—¥å¿—æ–‡ä»¶')
    parser.add_argument('--gnn_log', type=str,
                       default='experiment_plot_new/GNN_PPO_GAT_15_5.log', 
                       help='GNN-PPOæ—¥å¿—æ–‡ä»¶')
    parser.add_argument('--output_dir', type=str, default='plots/',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--plot_type', type=str, default='all',
                       choices=['curves', 'comparison', 'convergence', 'all'],
                       help='ç»˜åˆ¶å›¾è¡¨ç±»å‹')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ—¥å¿—æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    log_files = {
        'PPO Baseline': args.baseline_log,
        'GNN-PPO': args.gnn_log
    }
    
    existing_logs = {}
    for name, path in log_files.items():
        if os.path.exists(path):
            existing_logs[name] = path
        else:
            print(f"âš ï¸  æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {path}")
    
    if not existing_logs:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ—¥å¿—æ–‡ä»¶")
        return
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(existing_logs)} ä¸ªæ—¥å¿—æ–‡ä»¶ï¼Œå¼€å§‹ç”Ÿæˆå›¾è¡¨...")
    
    # ç”Ÿæˆå›¾è¡¨
    if args.plot_type in ['curves', 'all']:
        plot_training_curves(existing_logs, args.output_dir)
    
    if args.plot_type in ['comparison', 'all']:
        plot_performance_comparison(existing_logs, args.output_dir)
    
    if args.plot_type in ['convergence', 'all']:
        plot_convergence_analysis(existing_logs, args.output_dir)
    
    if args.plot_type == 'all':
        create_summary_report(existing_logs, args.output_dir)
    
    print("\\nâœ… å›¾è¡¨ç”Ÿæˆå®Œæˆï¼")


if __name__ == '__main__':
    main()